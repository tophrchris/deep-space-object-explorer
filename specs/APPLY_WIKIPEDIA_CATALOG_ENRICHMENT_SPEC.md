# `apply_wikipedia_catalog_enrichment.py` Specification

Status: normative for `scripts/apply_wikipedia_catalog_enrichment.py`.

This document defines how Wikipedia enrichment JSON is merged into a catalog cache Parquet file and metadata JSON.

## 1. Purpose

Apply enrichment results (typically generated by `scripts/build_wikipedia_catalog_enrichment.py`) to
`dso_catalog_cache.parquet` by:

1. Updating existing rows by `primary_id`.
2. Inserting missing rows when coordinate minimums are met.
3. Re-normalizing row types/order.
4. Updating cache metadata with merge stats.

## 2. Script and Runtime

- Script: `scripts/apply_wikipedia_catalog_enrichment.py`
- Entrypoint: `main()`
- Language/runtime: Python 3
- Required Python dependency: `pandas`
- Input format dependencies: Parquet reader support in pandas environment.

## 3. CLI Contract

Arguments and defaults:

1. `--catalog-cache-path` (default `data/dso_catalog_cache.parquet`)
2. `--enrichment-path` (default `data/wikipedia_catalog_enrichment.json`)
3. `--output-cache-path` (default unset, meaning overwrite `--catalog-cache-path`)
4. `--metadata-path` (default `data/dso_catalog_cache_meta.json`)
5. `--dry-run` (default false)

Behavior:

1. `--dry-run` computes and prints summary only; no Parquet/metadata writes.
2. Without `--dry-run`, Parquet is written first, then metadata is written.

## 4. Input Contracts

### 4.1 Catalog Cache Parquet Input

`--catalog-cache-path` must exist, else `FileNotFoundError`.

Input frame is normalized by `_prepare_frame(...)`:

1. Enforces canonical column set `CACHE_COLUMNS` (`REQUIRED + OPTIONAL + ENRICHED_OPTIONAL` from `catalog_ingestion`).
2. Adds missing columns with default values (`""` for string columns, `NaN` for numeric columns).
3. Reorders columns to canonical order.
4. Trims all string columns.
5. Coerces numeric columns:
   - `ra_deg`
   - `dec_deg`
   - `dist_value`
   - `redshift`
   - `ang_size_maj_arcmin`
   - `ang_size_min_arcmin`

Unlike `catalog_ingestion._normalize_frame`, this step does not drop rows with null coordinates or blank IDs.

### 4.2 Enrichment JSON Input

`--enrichment-path` must exist, else `FileNotFoundError`.

Hard payload requirement:

1. Top-level key `targets` must exist and be an array, else `ValueError`.

Each `targets[]` item is processed best-effort; invalid rows are skipped with counters.

## 5. Enrichment Row Parsing

For each target item:

1. `catalog_enrichment` object is read if present; else treated as `{}`.
2. Aliases are merged from:
   - `catalog_enrichment.aliases`
   - top-level `designations`
3. Emission lines from `catalog_enrichment.emission_lines`.
4. `info_url` fallback:
   - `catalog_enrichment.info_url`
   - else top-level `wikipedia_url`
5. `description` fallback:
   - `catalog_enrichment.description`
   - else top-level `description`
6. `image_url` fallback:
   - `catalog_enrichment.image_url`
   - else top-level `image_url`
7. Numeric parse uses `_coerce_float(...)` and yields `None` for non-numeric/blank/NaN.

Row eligibility (`_has_enrichment_payload`) is true when at least one of these is present:

1. non-empty `common_name`, `object_type`, `constellation`, `aliases`, `image_url`, `image_attribution_url`, `description`, `info_url`, `emission_lines`
2. numeric `ra_deg`, `dec_deg`, `ang_size_maj_arcmin`, or `ang_size_min_arcmin`

If not eligible, row is counted under `skipped_rows_without_payload`.

## 6. Merge Semantics

`primary_id` is trimmed for matching.

### 6.1 Existing Row Update

When `primary_id` already exists in cache:

String fields updated only when new value is non-empty and different:

1. `common_name`
2. `object_type`
3. `constellation`
4. `image_url`
5. `image_attribution_url`
6. `description`
7. `info_url`

Numeric fields updated only when new number is present and changed (`abs(delta) >= 1e-12`):

1. `ra_deg`
2. `dec_deg`
3. `ang_size_maj_arcmin`
4. `ang_size_min_arcmin`

List-merge fields:

1. `aliases`: merged unique, order-preserving, serialized with `;`
2. `emission_lines`: merged unique, order-preserving, serialized with `; `

Counters:

1. `field_updates[<field>]` increments per field mutation.
2. `rows_updated` increments once per changed row.

### 6.2 Insert New Row

When `primary_id` does not exist in initial cache map:

1. Require both `ra_deg` and `dec_deg`; otherwise count `skipped_inserts_missing_coordinates`.
2. Start from empty row template:
   - all string columns `""`
   - all numeric columns `NaN`
3. Populate:
   - `primary_id`
   - `catalog` from first non-empty:
     - top-level `item.catalog` uppercased
     - `catalog_enrichment.wikipedia_catalog` uppercased
     - inferred from `primary_id` (`NGC`/`IC`/`SH2`/`M`, else `WIKI`)
   - `common_name`
   - `object_type`
   - `ra_deg`, `dec_deg`
   - `ang_size_maj_arcmin`, `ang_size_min_arcmin`
   - `constellation`
   - `aliases` (serialized with `;`)
   - `object_type_group` from top-level `item.object_type_group`
   - `image_url`
   - `image_attribution_url`
   - `description`
   - `info_url`
   - `emission_lines` (serialized with `; `)

Counters:

1. `rows_inserted` increments per accepted insert candidate before final dedupe.

Important nuance:

1. The existing-ID index is built once from the original cache and is not updated during processing.
2. If the enrichment payload contains duplicate new `primary_id` rows, each can be counted in `rows_inserted`,
   but final dedupe will keep only first occurrence.

## 7. Post-Merge Normalization and Output Frame Rules

After updates/inserts:

1. `_prepare_frame(...)` re-applied (column enforcement + trim + numeric coercion).
2. Rows with blank `primary_id` removed.
3. Dedupe by `primary_id`, keep first.
4. Sort by `catalog`, then `primary_id`.

The resulting frame is the write payload for Parquet (unless `--dry-run`).

## 8. Summary Contract

A JSON summary is always printed to stdout (including dry-run), with keys:

1. `cache_path`
2. `enrichment_path`
3. `output_cache_path`
4. `targets_seen`
5. `rows_updated`
6. `rows_inserted`
7. `skipped_rows_without_payload`
8. `skipped_inserts_missing_coordinates`
9. `field_updates` (map)
10. `result_row_count`

## 9. Dry-Run Behavior

With `--dry-run`:

1. Summary is printed.
2. Message `[wikipedia-merge] dry-run enabled; no files written` is printed.
3. Script exits without writing Parquet or metadata.

## 10. Non-Dry Write Behavior

Without `--dry-run`:

1. Ensure output Parquet parent directory exists.
2. Write Parquet with `index=False`.
3. Print written Parquet path.

## 11. Metadata Update Contract

Metadata file behavior:

1. If metadata file exists and parses as JSON object, start from it.
2. Otherwise start from `{}`.

Updated/added top-level keys:

1. `cache` = output parquet path string
2. `loaded_at_utc` = current UTC ISO timestamp (second precision)
3. `row_count` = final row count
4. `catalog_counts` = value counts from final `catalog` column
5. `wikipedia_enrichment_merge` object:
   - `applied_at_utc`
   - `enrichment_path`
   - `targets_seen`
   - `rows_updated`
   - `rows_inserted`
   - `skipped_rows_without_payload`
   - `skipped_inserts_missing_coordinates`
   - `field_updates`

All existing unrelated metadata keys are preserved.

Metadata write:

1. Ensure metadata parent directory exists.
2. Write JSON with `indent=2`.

## 12. Field-Level Guarantees and Non-Guarantees

Guaranteed by this script:

1. Canonical column set/order per `catalog_ingestion` constants.
2. String trim + numeric coercion for canonical columns.
3. `primary_id` non-blank in output.
4. `primary_id` unique in output.
5. Sorted output by `catalog`, `primary_id`.

Not guaranteed by this script:

1. `ra_deg`/`dec_deg` non-null for all output rows (existing invalid rows are not explicitly dropped here).
2. `object_type_group` re-derivation from `object_type` (group is copied from payload on inserts only).
3. Recalculation of `license_label`, `dist_value`, `dist_unit`, `redshift`, `morphology`.

## 13. Upstream/Downstream Compatibility

Upstream producer compatibility:

1. Designed to consume output from `scripts/build_wikipedia_catalog_enrichment.py`.
2. Uses top-level fallback fields (`designations`, `wikipedia_url`, `description`, `image_url`) when catalog_enrichment
   keys are absent or blank.

Downstream consumer compatibility:

1. Writes Parquet compatible with expected app/catalog loader schema.
2. Metadata writes `wikipedia_enrichment_merge` summary for auditability.

## 14. Failure Modes

Hard failures (raise and exit non-zero):

1. Cache file missing.
2. Enrichment file missing.
3. Enrichment JSON malformed.
4. Enrichment payload missing valid `targets` array.
5. Parquet read/write errors from pandas backend/environment.
6. Metadata write filesystem errors.

Soft row-level failures (count-and-continue):

1. Non-dict target row.
2. Missing/blank `primary_id`.
3. Target with no meaningful enrichment payload.
4. Insert candidate missing required coordinates.

## 15. Operational Notes

1. Merge is additive/overwriting only for selected fields; it is not a full row replacement.
2. Existing string values are never replaced by blank values from enrichment payload.
3. Existing numeric values are only replaced when new numeric values are provided.
4. Alias/emission merges are order-preserving and deduplicated.
5. Because dedupe uses "keep first", target ordering in `targets[]` can affect which duplicate insert survives.

## 16. Normative Code References

- Schema column constants source: `dso_enricher/catalog_ingestion.py:15`, `dso_enricher/catalog_ingestion.py:24`, `dso_enricher/catalog_ingestion.py:33`
- Cache column/type setup in apply script: `scripts/apply_wikipedia_catalog_enrichment.py:20`
- Enrichment row parsing and fallback behavior: `scripts/apply_wikipedia_catalog_enrichment.py:107`
- Payload eligibility predicate: `scripts/apply_wikipedia_catalog_enrichment.py:149`
- Frame normalization in apply script: `scripts/apply_wikipedia_catalog_enrichment.py:178`
- CLI arguments: `scripts/apply_wikipedia_catalog_enrichment.py:192`
- Existing-row update logic: `scripts/apply_wikipedia_catalog_enrichment.py:275`
- Insert logic and coordinate gate: `scripts/apply_wikipedia_catalog_enrichment.py:325`
- Post-merge dedupe/sort: `scripts/apply_wikipedia_catalog_enrichment.py:358`
- Summary payload fields: `scripts/apply_wikipedia_catalog_enrichment.py:363`
- Parquet write path: `scripts/apply_wikipedia_catalog_enrichment.py:383`
- Metadata merge/write behavior: `scripts/apply_wikipedia_catalog_enrichment.py:387`
- Baseline catalog normalization for comparison: `dso_enricher/catalog_ingestion.py:311`
